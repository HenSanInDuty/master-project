{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "working_dir = os.path.dirname(os.path.abspath(''))\n",
    "data_process_dir = working_dir + \"\\\\src\\\\data\\\\processing-batch\\\\data-process\"\n",
    "stop_word_file = working_dir + \"\\\\src\\\\data\\\\processing-batch\\\\vietnamese-stopwords-dash.txt\"\n",
    "v_json_export_path = working_dir + \"\\\\src\\\\data\\\\processing-batch\\\\topic_model.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension = \".txt.pro\"\n",
    "delimiter_text = \"/\"\n",
    "noun_type = \"N\"\n",
    "topics = [\n",
    "    \"ThoiSu\",\n",
    "    \"TheGioi\",\n",
    "    \"KinhDoanh\",\n",
    "    \"BatDongSan\",\n",
    "    \"KhoaHoc\",\n",
    "    \"GiaiTri\",\n",
    "    \"TheThao\",\n",
    "    \"PhapLuat\",\n",
    "    \"GiaoDuc\",\n",
    "    \"SucKhoe\",\n",
    "    \"DoiSong\",\n",
    "    \"DuLich\",\n",
    "    \"SoHoa\",\n",
    "    \"Xe\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not TextIOWrapper",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Lấy ra các hư từ\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m stop_word_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstop_word_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stop_word_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      4\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m stop_words\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hen\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not TextIOWrapper"
     ]
    }
   ],
   "source": [
    "# Lấy ra các hư từ\n",
    "stop_word_file = open(stop_word_file, \"r\", encoding=\"utf-8\")\n",
    "stop_words = stop_word_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "stop_words.append(\"ảnh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_word(words: list):\n",
    "    noun_words = []\n",
    "    for text_split in words:\n",
    "        text_split = text_split\n",
    "        text_and_type = text_split.split(delimiter_text)\n",
    "        type = text_and_type.pop()\n",
    "        if type == noun_type:\n",
    "            text = text_and_type.pop()\n",
    "            # Nếu từ không phải hư từ thì đưa vào tập V\n",
    "            if text.lower() not in stop_words:\n",
    "                noun_words.append(text.lower())\n",
    "    return noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = {f\"{topic}\":[] for topic in topics}\n",
    "V = {f\"{topic}\":[] for topic in topics}\n",
    "N = {}\n",
    "n = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý lấy tất cả danh từ đưa vào tập V\n",
    "for topic in topics:\n",
    "    data_process_file_path = data_process_dir + f\"\\\\{topic}{extension}\"\n",
    "    with open(data_process_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "        # Xoá các ký tự xuống dòng\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        \n",
    "        # Tách các từ\n",
    "        text_splits = text.split(\" \")\n",
    "        \n",
    "        # Tách câu\n",
    "        sentences_splits = text.split(\"./.\")\n",
    "        \n",
    "        # Đưa các từ thuộc danh từ vào tập V\n",
    "        V[f'{topic}'] = get_noun_word(text_splits)\n",
    "        \n",
    "        # Đưa các câu vào tập S\n",
    "        for sentences_split in sentences_splits:\n",
    "            S[f'{topic}'].append(sentences_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tìm kiếm từ core cho mỗi chủ đề\n",
    "for topic in topics:\n",
    "    n = {}\n",
    "    for text in V[f'{topic}']:\n",
    "        if text in n.keys():\n",
    "            n[f\"{text}\"] += 1\n",
    "        else:\n",
    "            n[f\"{text}\"] = 1\n",
    "    \n",
    "    N[f'{topic}'] = [max(n, key=n.get), n[max(n, key=n.get)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ThoiSu': ['xe', 639],\n",
       " 'TheGioi': ['tổng_thống', 1587],\n",
       " 'KinhDoanh': ['công_ty', 106],\n",
       " 'BatDongSan': ['dự_án', 2283],\n",
       " 'KhoaHoc': ['công_nghệ', 772],\n",
       " 'GiaiTri': ['phim', 614],\n",
       " 'TheThao': ['trận', 1149],\n",
       " 'PhapLuat': ['công_ty', 741],\n",
       " 'GiaoDuc': ['trường', 2268],\n",
       " 'SucKhoe': ['bác_sĩ', 951],\n",
       " 'DoiSong': ['nhân_viên', 50],\n",
       " 'DuLich': ['du_khách', 1417],\n",
       " 'SoHoa': ['công_ty', 613],\n",
       " 'Xe': ['xe', 2992]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshhold = 0\n",
    "\n",
    "# Cập nhật lại V cho các chủ đề\n",
    "V = {f\"{topic}\":[] for topic in topics}\n",
    "for topic in topics:\n",
    "    n = {}\n",
    "    \n",
    "    # Lấy ra từ core\n",
    "    core_word = N[f'{topic}']\n",
    "    \n",
    "    for sentence in S[f'{topic}']:\n",
    "        # Tách các từ\n",
    "        words = sentence.split(\" \")\n",
    "        \n",
    "        \n",
    "        # Lấy ra các danh từ\n",
    "        noun_words = get_noun_word(words)\n",
    "        \n",
    "        # Nếu như core word có mặt trong câu thì cộng lại số lượng những danh từ trong đó\n",
    "        if core_word[0] in noun_words:\n",
    "            for noun_word in noun_words:\n",
    "                if noun_word in n.keys():\n",
    "                    n[f\"{noun_word}\"] += 1\n",
    "                else:\n",
    "                    n[f\"{noun_word}\"] = 1\n",
    "        \n",
    "    # Nếu trong 1 câu cùng xuất hiện từ chủ đề và 1 danh từ thì đưa danh từ vào tập chủ đề đó dựa trên threshold\n",
    "    for word, frequency in n.items():\n",
    "        if frequency/core_word[1] > threshhold:\n",
    "            V[f'{topic}'].append([word.lower(), frequency])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export thành file json\n",
    "# Serializing json\n",
    "json_object = json.dumps(V, indent=4)\n",
    " \n",
    "# Writing to sample.json\n",
    "with open(v_json_export_path,\"w\" , encoding=\"utf-8\") as outfile:\n",
    "    json.dump(V, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
