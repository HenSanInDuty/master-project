{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path\n",
    "working_dir = os.path.dirname(os.path.abspath(''))\n",
    "v_json_export_path = working_dir + \"\\\\src\\\\data\\\\processing-batch\\\\topic_model.json\"\n",
    "batch_dir = working_dir + \"\\\\src\\\\data\\\\processing-batch\"\n",
    "bat_file_path = batch_dir + \"\\\\Tokenizer.bat\"\n",
    "jvn_path = batch_dir + \"\\\\JVnTextPro-3.0.3-executable.jar\"\n",
    "stop_word_file = batch_dir + \"\\\\vietnamese-stopwords-dash.txt\"\n",
    "pmi_path = batch_dir + \"\\\\pmi.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load topic model \n",
    "with open(v_json_export_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    topic_model = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pmi cho các từ\n",
    "with open(pmi_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    PMI = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy ra các hư từ\n",
    "stop_word_file = open(stop_word_file, \"r\", encoding=\"utf-8\")\n",
    "stop_words = stop_word_file.read()\n",
    "stop_words = stop_words.split(\"\\n\")\n",
    "stop_words.append(\"ảnh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noun_word(words: list, noun_type: str, delimiter_text: str, stop_words: list) -> list:\n",
    "    '''\n",
    "    Lấy ra các danh từ trong tập các từ\n",
    "    \n",
    "    Param:\n",
    "    --------------------------------------------\n",
    "    words: tất cả các từ trong văn bản, còn chứa pos\n",
    "    noun_type: loại danh từ cần tách\n",
    "    delimiter_text: ký hiệu phân loại của từ\n",
    "    stop_words: danh sách các hư từ\n",
    "    \n",
    "    Return:\n",
    "    ----------------------------------------------\n",
    "    Trả về list các từ có loại noun_type đã được chuyển về chữ thường\n",
    "    '''\n",
    "    \n",
    "    noun_words = []\n",
    "    for text_split in words:\n",
    "        text_split = text_split\n",
    "        text_and_type = text_split.split(delimiter_text)\n",
    "        type = text_and_type.pop()\n",
    "        if type == noun_type:\n",
    "            text = text_and_type.pop()\n",
    "            # Nếu từ không phải hư từ thì đưa vào tập noun_word\n",
    "            if text.lower() not in stop_words:\n",
    "                noun_words.append(text.lower())\n",
    "    return noun_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging_document(document:str, file_input_name: str | None = None):\n",
    "    ''' \n",
    "    Đánh dấu từ loại cho văn bảng\n",
    "    \n",
    "    Param\n",
    "    ------------------------------------------\n",
    "    document: văn bảng dạng text hoặc file\n",
    "    file_input_name: nếu là None thì văn bảng là dạng text và ngược lại\n",
    "    '''\n",
    "    \n",
    "     # Chuyển text thành file để xử lý bằng jvnpro\n",
    "    if file_input_name == None:\n",
    "        file_input_name = f\"Temp{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "        input_file_name = batch_dir + f\"\\\\data-process\\\\{file_input_name}.txt\"\n",
    "        document = document.replace(\"\\n\", \" \")\n",
    "    \n",
    "        # Chuyển văn bảng ra thành file\n",
    "        with open(input_file_name, \"x\", encoding=\"utf-8\") as file:\n",
    "            file.write(document)\n",
    "    # TODO: xử lý văn bảng dạng file\n",
    "        \n",
    "    subprocess.call([bat_file_path, jvn_path, input_file_name])\n",
    "    \n",
    "    # File output của tool\n",
    "    tagging_document = \"\"\n",
    "    input_file_name_pro = batch_dir + f\"\\\\data-process\\\\{file_input_name}.txt.pro\"\n",
    "    with open(input_file_name_pro, \"r\", encoding=\"utf-8\") as output_file:\n",
    "        tagging_document = output_file.read()\n",
    "    \n",
    "    # Xoá các file được tạo ra bởi tool\n",
    "    os.remove(input_file_name)\n",
    "    os.remove(input_file_name_pro)\n",
    "    \n",
    "    return tagging_document.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_all_terminology(tagging_document: str, topic_model: dict, topic: str | None) -> dict:\n",
    "    '''\n",
    "    Đếm số lần xuất hiện của các thuật ngữ và trả về dict\n",
    "    \n",
    "    Param\n",
    "    -----------------------------------------\n",
    "    tagging_document: văn bản đã được đánh dấu từ loại\n",
    "    topic: chủ đề của văn bản (nếu không biết trước thì đặt là None)\n",
    "    \n",
    "    Return\n",
    "    -----------------------------------------\n",
    "    dict: chứa số lần xuất hiện của tất cả thuật ngữ \n",
    "    {terminology: count, all: count}\n",
    "    '''\n",
    "    \n",
    "    terminologys = {}\n",
    "    \n",
    "    words =  tagging_document.split(\" \")\n",
    "    noun_words = get_noun_word(words, \"N\", \"/\", stop_words)\n",
    "    \n",
    "    # TODO: xử lý khi không có topic cụ thể\n",
    "    if topic == None:\n",
    "        pass\n",
    "    else:\n",
    "        # Đếm các thuật ngữ có trong văn bản\n",
    "        model_terminologys = [word[0] for word in topic_model[f'{topic}']]\n",
    "        count_all = 0\n",
    "        for noun in noun_words: \n",
    "            if noun in model_terminologys:\n",
    "                count_all += 1\n",
    "                if noun in terminologys.keys():\n",
    "                    terminologys[f'{noun}'] += 1\n",
    "                else:\n",
    "                    terminologys[f'{noun}'] = 1\n",
    "    \n",
    "    terminologys['terminologies_count_all'] = count_all\n",
    "    return terminologys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_terminology(sentences: str, terminology_count: dict) -> float:\n",
    "    '''\n",
    "    Tính trọng số của các thuật ngữ trong một câu\n",
    "    \n",
    "    Param\n",
    "    ---------------------------------------------\n",
    "    sentences: câu cần tính trọng số\n",
    "    terminology_count: tập các thuật ngữ\n",
    "    \n",
    "    Return\n",
    "    --------------------------------------------\n",
    "    Trọng số: float\n",
    "    '''\n",
    "    words = sentences.split(\" \")\n",
    "    noun_word = get_noun_word(words, \"N\", \"/\", stop_words)\n",
    "    \n",
    "    # Trọng số của câu\n",
    "    weight = 0\n",
    "    \n",
    "    # Lấy ra các thuật ngữ trong câu\n",
    "    noun_word = [noun for noun in noun_word if noun in terminology_count.keys()]\n",
    "    \n",
    "    # Không có thuật ngữ nào trong câu\n",
    "    if len(noun_word) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Tính trọng số cho từng thuật ngữ\n",
    "    noun_word.sort()\n",
    "    noun_weight = 1\n",
    "    noun_current = noun_word[0]\n",
    "    for i in range(1, len(noun_word)):\n",
    "        if noun_word[i] != noun_current:\n",
    "            # Số lần xuất hiện của thuật ngữ chia cho toàn bộ số lần xuất hiện của các thuật ngữ khác\n",
    "            weight += noun_weight / terminology_count['terminologies_count_all']\n",
    "            noun_current = noun_word[i]\n",
    "            noun_weight = 1\n",
    "        else:\n",
    "            noun_weight += 1\n",
    "    weight += noun_weight / terminology_count['terminologies_count_all']\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_sentences_weight(tagging_document: str, terminology_count:dict) -> list:\n",
    "    '''\n",
    "    Tính trọng số của các câu trong văn bản\n",
    "    \n",
    "    Params\n",
    "    ----------------------------\n",
    "    terminology_word: thuật ngữ trong câu\n",
    "    terminology_count: các thuật ngữ có trong văn bản\n",
    "    \n",
    "    Return\n",
    "    -----------------------------\n",
    "    Trả về list có định dạng như sau:\n",
    "    [\n",
    "        {\n",
    "            id: id của câu,\n",
    "            content: nội dung câu,\n",
    "            weight: trọng số của câu dựa vào thuật ngữ\n",
    "        }\n",
    "    ]\n",
    "    '''\n",
    "    \n",
    "    sentences = tagging_document.split(\"./.\")\n",
    "    weighted_sentences = []\n",
    "    for i in range(len(sentences)):\n",
    "        weight = weight_terminology(sentences[i], terminology_count)\n",
    "        \n",
    "        weighted_sentences.append({\n",
    "            'id' : f'S{i+1}',\n",
    "            'content': sentences[i],\n",
    "            'weight': weight,\n",
    "        })\n",
    "    \n",
    "    return weighted_sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = '''Ngày 15/8, Ban Quản lý rừng phòng hộ huyện Nghi Lộc cho biết dịch sâu róm xuất hiện tại cánh rừng thông trên địa bàn từ giữa tháng 7. Vòng đời sâu róm khoảng 50 ngày, hiện đã đến thế hệ thứ ba.\n",
    "\n",
    "Đến nay, 300 ha rừng thông ở các xã Nghi Yên, Nghi Tiến, Nghi Quang, Nghi Xá bị sâu ăn trụi lá, mật độ khoảng 350-400 con một cây. 450 ha rừng thông rải rác trải dài trên 17 xã khác trong vùng cũng bị sâu tấn công với mức độ trung bình, mật độ 150-200 con một cây. Một số điểm nhỏ lẻ là 10-30 con một cây.\n",
    "Ông Trần Văn Trường, Phó ban Quản lý rừng phòng hộ huyện Nghi Lộc, cho biết vừa qua trời nhiều sương mù, nắng mưa thất thường, độ ẩm cao, tạo điều kiện cho sâu róm sinh trưởng mạnh. Hiện sâu ăn, cắn phá lá thông một chu kỳ, nếu để sâu phát triển thêm nhiều chu kỳ sẽ có thể gây chết cây.\n",
    "\n",
    "\"Chúng tôi đã cử cán bộ phun thuốc trên diện tích 142 ha. Thời điểm này sâu bắt đầu đóng kén, chuẩn bị vòng đời mới nên tạm dừng phun. Đơn vị đang chuẩn bị đèn và các vật dụng làm điểm đèn để bẫy, bắt sâu trưởng thành nở ra bướm\", ông Trường nói.\n",
    "Ban Quản lý rừng phòng hộ huyện Nghi Lộc được giao nhiệm vụ quản lý, bảo vệ và phát triển hơn 5.000 ha đất lâm nghiệp gồm đất rừng quy hoạch phòng hộ, rừng sản xuất, trải dài trên 17 xã.\n",
    "\n",
    "Sâu róm hay còn gọi là sâu lông, khi trưởng thành có lông chứa độc tố.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tagging\n",
    "tagging_document = pos_tagging_document(document, None)\n",
    "# Đếm các thuật ngữ\n",
    "terminologies = count_all_terminology(tagging_document, topic_model, 'ThoiSu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'S1',\n",
       "  'content': 'Ngày/N 15/8/M ,/, Ban_Quản_lý/N rừng_phòng_hộ/N huyện/N Nghi_Lộc/Np cho_biết/Np dịch/N sâu_róm/N xuất_hiện/V tại/E cánh/N rừng/N thông/A trên/A địa_bàn/N từ/E giữa/M tháng/N 7/M ',\n",
       "  'weight': 0.125},\n",
       " {'id': 'S2',\n",
       "  'content': ' Vòng/N đời/N sâu_róm/N khoảng/N 50/M ngày/N ,/, hiện/Np đã/R đến/V thế_hệ/N thứ_ba/N ',\n",
       "  'weight': 0.03125},\n",
       " {'id': 'S3',\n",
       "  'content': ' Đến/E nay/P ,/, 300/M ha/Nu rừng/N thông/A ở/E các/L xã/N Nghi/Np Yên/Np ,/, Nghi_Tiến/Np ,/, Nghi/Np Quang/Np ,/, Nghi/Np Xá/Np bị/V sâu/A ăn/V trụi/A lá/A ,/, mật_độ/N khoảng/N 350-400/M con/Nc một/M cây/X ',\n",
       "  'weight': 0.09375},\n",
       " {'id': 'S4',\n",
       "  'content': ' 450/M ha/Nu rừng/N thông/V rải_rác/A trải/V dài/A trên/A 17/M xã/N khác/A trong_vùng/A cũng/R bị/V sâu/A tấn_công/V với/E mức_độ/N trung_bình/A ,/, mật_độ/N 150-200/M con/Np một/M cây/X ',\n",
       "  'weight': 0.125},\n",
       " {'id': 'S5',\n",
       "  'content': ' Một_số_điểm/Nc nhỏ/N lẻ/Np là/C 10-30/M con/Np một/M cây/X ',\n",
       "  'weight': 0},\n",
       " {'id': 'S6',\n",
       "  'content': ' Ông/Nc Trần_Văn_Trường/Np ,/, Phó/Np ban_Quản_lý/V rừng_phòng_hộ/N huyện/N Nghi_Lộc/Np ,/, cho_biết/V vừa_qua/X trời/N nhiều/A sương_mù/N ,/, nắng_mưa/N thất_thường/A ,/, độ/N ẩm/A cao/A ,/, tạo_điều_kiện/N cho/E sâu_róm/N sinh_trưởng/V mạnh/A ',\n",
       "  'weight': 0.125},\n",
       " {'id': 'S7',\n",
       "  'content': ' Hiện/N sâu/A ăn/V ,/, cắn/V phá/V lá_thông/N một/M chu_kỳ/N ,/, nếu/C để/E sâu/V phát_triển/V thêm/V nhiều/A chu_kỳ/N sẽ/R có_thể/R gây/V chết/V cây./X \"/\" Chúng_tôi/P đã/R cử/V cán_bộ/N phun_thuốc/Np trên/A diện_tích/N 142/M ha/Nu ',\n",
       "  'weight': 0.15625},\n",
       " {'id': 'S8',\n",
       "  'content': ' Thời_điểm/N này/P sâu/A bắt_đầu/V đóng/V kén/N ,/, chuẩn_bị/V vòng/N đời_mới/N nên/V tạm/A dừng/V phun/V ',\n",
       "  'weight': 0},\n",
       " {'id': 'S9',\n",
       "  'content': ' Đơn_vị/Np đang/R chuẩn_bị/V đèn/N và/C các/L vật_dụng/N làm/V điểm/N đèn/N để/E bẫy/V ,/, bắt_sâu/N trưởng_thành/V nở/V ra/E bướm/N \"/\" ,/, ông/Nc Trường/N nói/V ',\n",
       "  'weight': 0.125},\n",
       " {'id': 'S10',\n",
       "  'content': ' Ban_Quản_lý/N rừng_phòng_hộ/N huyện/N Nghi_Lộc/Np được/V giao/V nhiệm_vụ/N quản_lý/V ,/, bảo_vệ/N và/C phát_triển/V hơn/A 5.000/M ha/Nu đất/N lâm_nghiệp/N gồm/V đất_rừng/N quy_hoạch/V phòng/N hộ/A ,/, rừng/N sản_xuất/V ,/, trải/V dài/A trên/A 17/M xã/N ',\n",
       "  'weight': 0.21875},\n",
       " {'id': 'S11',\n",
       "  'content': ' Sâu_róm/Np hay/C còn/C gọi_là/C sâu/A lông/N ,/, khi/N trưởng/Np thành/Np có/T lông/N chứa/V độc_tố/N ',\n",
       "  'weight': 0},\n",
       " {'id': 'S12', 'content': ' ', 'weight': 0}]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tính trọng số của câu\n",
    "sentences_weights = cal_sentences_weight(tagging_document, terminologies)\n",
    "sentences_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lấy ra các câu trong văn bản\n",
    "sentences = tagging_document.split(\"./.\")\n",
    "# Tính độ tương đồng của câu\n",
    "sentences_similarity = {}\n",
    "max = 0\n",
    "min = 0\n",
    "for i in range(len(sentences) - 1):\n",
    "    for j in range(1, len(sentences)):\n",
    "        similarity = 0\n",
    "        words_sen1 = get_noun_word(sentences[i].split(\" \"),\"N\",\"/\",stop_words)\n",
    "        words_sen2 = get_noun_word(sentences[j].split(\" \"),\"N\",\"/\",stop_words)\n",
    "        words_sen1.sort()\n",
    "        words_sen2.sort()\n",
    "        for word1 in words_sen1:\n",
    "            for word2 in words_sen2:\n",
    "                word_pair = word1 + \" \" + word2\n",
    "                if word_pair in PMI.keys():\n",
    "                    similarity += PMI[f'{word_pair}']\n",
    "                    \n",
    "        if max < similarity:\n",
    "            max = similarity\n",
    "        \n",
    "        if min > similarity:\n",
    "            min = similarity\n",
    "        sentences_similarity[f'S{i+1} S{j+1}'] = similarity\n",
    "\n",
    "# Chuẩn hoá lại độ tương đồng\n",
    "for key, value in sentences_similarity.items():\n",
    "    sentences_similarity[f'{key}'] = (value - min) / (max - min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_list_summary_sentences(summary_sentences: list) -> str:\n",
    "    '''\n",
    "    Cộng chuỗi các câu để tóm tắt\n",
    "    \n",
    "    Param\n",
    "    -------------------------------\n",
    "    summary_sentences: danh sách các câu\n",
    "    \n",
    "    Return\n",
    "    -------------------------------\n",
    "    str: Văn bảng tóm tắt theo thứ tự\n",
    "    '''\n",
    "    \n",
    "    summary_document = ''\n",
    "    summary_sentences = sorted(summary_sentences, key=lambda x:x[0])\n",
    "    \n",
    "    for summary_sentence in summary_sentences:\n",
    "        summary_document += summary_sentence[1] + '. '\n",
    "    \n",
    "    return summary_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xây dựng mô hình đồ thị\n",
    "# Tỉ lệ tóm tắt\n",
    "r_threshold = 0.4\n",
    "\n",
    "# Tỉ lệ tương đồng\n",
    "alpha = 0.2\n",
    "\n",
    "sentences = tagging_document.split(\"./.\")\n",
    "sorted_sentences_weights = sorted(sentences_weights, key=lambda x:x['weight'], reverse=True)\n",
    "choice_sentences = []\n",
    "final_summary = []\n",
    "for sentence_weight in sorted_sentences_weights:\n",
    "    if len(concat_list_summary_sentences(final_summary))/len(document) > r_threshold:\n",
    "        break \n",
    "    sentence_index = int(sentence_weight['id'][1:]) - 1\n",
    "    sentence = sentences[sentence_index]\n",
    "    \n",
    "    # Xác định xem có dùng câu này không\n",
    "    ok = False\n",
    "    if len(choice_sentences) == 0:\n",
    "        ok = True\n",
    "    else:\n",
    "        # Kiểm tra độ tương đồng của các câu\n",
    "        for s_choice_index in choice_sentences:\n",
    "            if (s_choice_index > sentence_index):\n",
    "                id = f'S{sentence_index + 1} S{s_choice_index + 1}'\n",
    "            else:\n",
    "                id = f'S{s_choice_index + 1} S{sentence_index + 1}'\n",
    "        \n",
    "        if sentences_similarity[f'{id}'] < alpha:\n",
    "            ok = True\n",
    "    \n",
    "    if ok:\n",
    "        choice_sentences.append(sentence_index)\n",
    "        # Xoá các loại từ và dấu _\n",
    "        original_sentence = \" \"\n",
    "        for word in sentence.split(\" \"):\n",
    "            original_sentence += word.split(\"/\")[0].replace('_', ' ') + ' '\n",
    "        final_summary.append([sentence_index, original_sentence.strip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[9,\n",
       "  'Ban Quản lý rừng phòng hộ huyện Nghi Lộc được giao nhiệm vụ quản lý , bảo vệ và phát triển hơn 5.000 ha đất lâm nghiệp gồm đất rừng quy hoạch phòng hộ , rừng sản xuất , trải dài trên 17 xã'],\n",
       " [8,\n",
       "  'Đơn vị đang chuẩn bị đèn và các vật dụng làm điểm đèn để bẫy , bắt sâu trưởng thành nở ra bướm \" , ông Trường nói'],\n",
       " [2,\n",
       "  'Đến nay , 300 ha rừng thông ở các xã Nghi Yên , Nghi Tiến , Nghi Quang , Nghi Xá bị sâu ăn trụi lá , mật độ khoảng 350-400 con một cây'],\n",
       " [1, 'Vòng đời sâu róm khoảng 50 ngày , hiện đã đến thế hệ thứ ba'],\n",
       " [4, 'Một số điểm nhỏ lẻ là 10-30 con một cây']]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_list_summary_sentences(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Person\\CaoHoc\\Project_ForAll\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.7095])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bert_score import BERTScorer\n",
    "from transformers.models.bert.modeling_bert import BertModel,BertForMaskedLM\n",
    "scorer = BERTScorer(lang='vi')\n",
    "_, _, F1_1 = scorer.score([\"Nhan\"], [\"N\"])\n",
    "F1_1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
