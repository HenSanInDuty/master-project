import json
import streamlit as st
import pandas as pd
import asyncio

from utils import api

st.set_page_config(
    layout="wide"
)

# ---------- T·∫°o session state ----------
# API state
if 'api_data_trainning' not in st.session_state:
    st.session_state.api_data_trainning = {
        'finish' : False
    }
# API state
if 'api_data_inference' not in st.session_state:
    st.session_state.api_data_inference = {
        'finish' : False
    }
# Widget state
if 'btn_result' not in st.session_state:
    st.session_state.btn_result = False
    
# ---------- Ch∆∞∆°ng tr√¨nh ch√≠nh ----------
async def main():
    st.title("T√≥m t·∫Øt ƒëa vƒÉn b·∫£n ti·∫øng Vi·ªát üìë")

    mode_choice = ['Trainning', 'Inference', 'Evaluate']

    mode = st.selectbox("Ch·ªçn ch·∫ø ƒë·ªô cho ch∆∞∆°ng tr√¨nh", mode_choice, index = None)

    # ====================== Trainning Module ======================
    if mode == mode_choice[0]:
        # Ch∆∞a g·ªçi API th√¨ hi·ªán c√°i v√≤ng tr√≤n l√™n
        if not st.session_state.api_data_trainning['finish']:
            with st.spinner("ƒê·ª£i x√≠u, ƒëang l·∫•y d·ªØ li·ªáu"):
                st.session_state.api_data_trainning['pos_tools'] = await api.get_pos_tools()
                st.session_state.api_data_trainning['word_similarity_tools'] = await api.get_word_similarity_tools()
                st.session_state.api_data_trainning['finish'] = True
                
        st.subheader("Hu·∫•n luy·ªán m√¥ h√¨nh üèãüèº‚Äç‚ôÇÔ∏è")
        
        # ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£n theo ƒë·ªãnh d·∫°ng cho tr∆∞·ªõc
        file_hint_col1, file_hint_col2 = st.columns(2)
        with file_hint_col1:
            st.write("ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£ng theo ƒë·ªãnh d·∫°ng json")
        with file_hint_col2:
            with st.popover("B·∫•m v√†o ƒë·ªÉ xem d·ªãnh d·∫°ng json", use_container_width=True):
                st.markdown('''```json
                            [{'topic':'Ch·ªß ƒë·ªÅ c·ªßa vƒÉn b·∫£n', 'content':'N·ªôi dung c·ªßa vƒÉn b·∫£n'}]''')
        document_uploaded_json = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files= False, key="document_uploaded")
        
        # -- Ch·ªçn t·∫≠p h∆∞ t·ª´, c√≥ th·ªÉ th√™m file ho·∫∑c l√† th√™m vƒÉn b·∫£ng ho·∫∑c kh√¥ng c·∫ßn h∆∞ t·ª´
        st.write("ƒê∆∞a v√†o t·∫≠p h∆∞ t·ª´ c√≥ t·ª´ gh√©p v·ªõi d∆∞·ªõi d·∫°ng _. V√≠ d·ª•: h·ªçc_sinh")
        stop_word_text_file = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files = False, key="stop_words_uploaded")
        
        # -- Ch·ªçn tool ƒë·ªÉ th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i
        st.write("Ch·ªçn c√¥ng c·ª• th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i")
        
        # G·ªçi api ƒë·ªÉ l·∫•y danh s√°ch c√°c tool c√≥ th·ªÉ s·ª≠ d·ª•ng
        pos_tools = st.selectbox("Ch·ªçn c√¥ng c·ª•", st.session_state.api_data_trainning['pos_tools'])
        
        # ------ Trainning Topic Model
        st.markdown("#### Ph·∫ßn c·∫•u h√¨nh d√†nh cho m√¥ h√¨nh ch·ªß ƒë·ªÅ")
        
        # Ch·ªçn ng∆∞·ª°ng cho c√°c t·ª´ ch·ªß ƒë·ªÅ (s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ / s·ªë l·∫ßn xu·∫•t hi·ªán c·ªßa t·ª´ core)
        topic_word_threshold = st.number_input("Ch·ªçn ng∆∞·ª°ng d√†nh cho t·ª´ ch·ªß ƒë·ªÅ", 0.0, 1.0, step=1.0, format="%.2f")
        topic_word_threshold = round(topic_word_threshold, 2)
        
        # ------ Trainning PMI 
        st.markdown("#### Ph·∫ßn c·∫•u h√¨nh d√†nh cho t√≠nh to√°n ƒë·ªô t∆∞∆°ng t·ª± c·ªßa t·ª´")
        word_similarity_tools = st.selectbox("Ch·ªçn c√¥ng c·ª•", st.session_state.api_data_trainning['word_similarity_tools'], key="word_similarity_tools_trainning")
        
        # Trainning
        if st.button("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán"):
            with st.spinner("ƒêang hu·∫•n luy·ªán"):
                try:
                    trainning_option = {
                        'pos_tool': pos_tools,
                        'threshold_core_word': topic_word_threshold,
                        'similarity_calculation_method': word_similarity_tools
                    }
                    result_trainning = await api.trainning(document_uploaded_json, stop_word_text_file, trainning_option)
                    st.success(f"ƒê√£ ho√†n th√†nh hu·∫•n luy·ªán")
                    
                    # # Hi·ªÉn th·ªã 2 ƒë·ªô ƒëo
                    # st.write("ƒê√°nh gi√° m√¥ h√¨nh")
                    # result = pd.DataFrame.from_dict(result_trainning['evaluate'], orient='index', columns=['Max','Min','Mean'])
                    # st.dataframe(result)
                    
                    # T·∫£i m√¥ h√¨nh v·ªÅ
                    st.download_button(
                        "·∫§n v√†o ƒë·ªÉ t·∫£i m√¥ h√¨nh v·ªÅ",
                        data=json.dumps(result_trainning),
                        file_name="topic_model.json",
                        mime="application/json",
                    )
                except Exception as e:
                    st.error("C√≥ l·ªói trong l√∫c hu·∫•n luy·ªán")
                    st.error(e)
                    
    # ====================== Inference Module ======================
    elif mode == mode_choice[1]:
        # Ch∆∞a g·ªçi API th√¨ hi·ªán c√°i v√≤ng tr√≤n l√™n
        if not st.session_state.api_data_inference['finish']:
            with st.spinner("ƒê·ª£i x√≠u, ƒëang l·∫•y d·ªØ li·ªáu"):
                st.session_state.api_data_inference['pos_tools'] = await api.get_pos_tools()
                st.session_state.api_data_inference['finish'] = True
        # ----- ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£ng
        # Th√™m vƒÉn b·∫£n
        st.subheader("T·∫£i l√™n t·∫≠p vƒÉn b·∫£n ƒë·ªÉ th·ª±c hi·ªán t√≥m t·∫Øt")
        file_hint_col1, file_hint_col2 = st.columns(2)
        with file_hint_col1:
            st.write("ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£ng theo ƒë·ªãnh d·∫°ng json")
        with file_hint_col2:
            with st.popover("B·∫•m v√†o ƒë·ªÉ xem d·ªãnh d·∫°ng json", use_container_width=True):
                st.markdown('''```json
[{'topic':'Ch·ªß ƒë·ªÅ c·ªßa vƒÉn b·∫£n',
'content':'N·ªôi dung c·ªßa vƒÉn b·∫£n'}]''')
        document_uploaded_json = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files= False, key="document_uploaded")
        
        # -- Ch·ªçn tool ƒë·ªÉ th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i
        st.write("Ch·ªçn c√¥ng c·ª• th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i")
        
        # G·ªçi api ƒë·ªÉ l·∫•y danh s√°ch c√°c tool c√≥ th·ªÉ s·ª≠ d·ª•ng
        pos_tools = st.selectbox("Ch·ªçn c√¥ng c·ª•", st.session_state.api_data_inference['pos_tools'])
        
        # ----- ƒê∆∞a v√†o m√¥ h√¨nh ch·ªß ƒë·ªÅ
        st.subheader("T·∫£i l√™n m√¥ h√¨nh (d·∫°ng json)")
        model_hint_col1, model_hint_col2 = st.columns(2)
        with model_hint_col1:
            st.write("M√¥ h√¨nh c√≥ d·∫°ng json")
        with model_hint_col2:
            with st.popover("B·∫•m v√†o ƒë·ªÉ xem d·ªãnh d·∫°ng json", use_container_width=True):
                st.markdown('''```json
{'topic_model':{'ch·ªß ƒë·ªÅ': [t·ª´ c·ªët l√µi]},
'word_similarity':{'type': 'lo·∫°i c√¥ng c·ª• cho ƒë·ªô t∆∞∆°ng t·ª± t·ª´', 
                    'word_similarity': 'k·∫øt qu·∫£ c·ªßa c√¥ng c·ª•',
                    'stop_words': 'danh s√°ch h∆∞ t·ª´ cho m√¥ h√¨nh'}}''')
        model_uploaded_json = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files= False, key="model_uploaded")
        
        # ----- Nh·∫≠p c√°c th√¥ng s·ªë
        st.subheader("C√°c th√¥ng s·ªë t√≥m t·∫Øt")
        
        # T·ªâ l·ªá t√≥m t·∫Øt
        r_threshold = st.number_input("Ch·ªçn ƒë·ªô n√©n (t·ªâ l·ªá t√≥m t·∫Øt)", 0.0, 1.0, step=1.0, format="%.2f")
        r_threshold = round(r_threshold, 2)
        
        # Comment l·∫°i v√¨ kh√¥ng c√≤n d√πng n·ªØa -> chuy·ªÉn qua d√πng GA cho b√†i to√°n t·ªëi ∆∞u
        # # T·ªâ l·ªá t∆∞∆°ng ƒë·ªìng c·ªßa c√¢u
        # similarity_word = st.number_input("Ch·ªçn ng∆∞·ª°ng d√†nh cho t·ªâ l·ªá t∆∞∆°ng ƒë·ªìng c·ªßa c√¢u", 0.0, 1.0, step=1.0, format="%.2f")
        # similarity_word = round(similarity_word, 2)
        
        if st.button("B·∫Øt ƒë·∫ßu t√≥m t·∫Øt") or st.session_state.btn_result:
            # ----- K·∫øt qu·∫£
            with st.spinner("ƒêang ti·∫øn h√†nh t√≥m t·∫Øt vƒÉn b·∫£n"):
                result = await api.inference(document_uploaded_json,
                                            model_uploaded_json,
                                            {'r_threshold':r_threshold, 'pos_tool':pos_tools})
                st.session_state.btn_result = True
                
                # Style cho disable textarea
                st.markdown("""
                <style>
                .stTextArea [data-baseweb=base-input] [disabled=""]{
                    -webkit-text-fill-color: black;
                }
                </style>
                """,unsafe_allow_html=True)
                
                # VƒÉn b·∫£n sau khi t√≥m t·∫Øt
                st.subheader("K·∫øt qu·∫£ t√≥m t·∫Øt vƒÉn b·∫£n")
                summarized_text = result['summary_document']
                st.text_area("",value=summarized_text,height=300,disabled=True)

                # T·∫£i t·∫≠p tin t√≥m t·∫Øt
                st.download_button("T·∫£i v·ªÅ n·ªôi dung t√≥m t·∫Øt", summarized_text)
                    
                # ----- B·∫£ng th√¥ng tin t√≥m t·∫Øt c·ªßa vƒÉn b·∫£n 
                tab1, tab2, tab3 = st.tabs(["Danh s√°ch c√¢u", "Th√¥ng tin c√¢u", "Th√¥ng tin ƒë·ªô t∆∞∆°ng ƒë·ªìng"])

                # -- Tab danh s√°ch c√°c c√¢u
                with tab1:
                    sentences = result["sentences"]

                    for i, sentence in enumerate(sentences):
                        # font_large = f'<p style="font-size: 24px;"></p>'
                        font_large = f'{i + 1}. {sentence.replace("\n", "")}'
                        st.markdown(font_large, unsafe_allow_html=False)

                # -- Tab th√¥ng tin c·ªßa c√¢u (tr·ªçng s·ªë)
                with tab2:
                    ratio_tab2 = [1, 3]
                    # Chia ra l√†m 2 c·ªôt, c·ªôt ƒë·∫ßu l√† s·ªë th·ª© t·ª± c√¢u, c·ªôt 2 l√† tr·ªçng s·ªë c·ªßa c√¢u
                    header_tab2_col1, header_tab2_col2 = st.columns(ratio_tab2, vertical_alignment="center")
                    with header_tab2_col1:
                        st.write("C√¢u")
                    with header_tab2_col2:
                        st.write("Tr·ªçng s·ªë")
                        
                    for i in range(result['number_sentence']):
                        tab2_col1, tab2_col2 = st.columns(ratio_tab2, vertical_alignment="center")
                        with tab2_col1:
                            with st.popover(f"C√¢u {i+1}"):
                                st.write(result['sentences'][i])
                            
                        with tab2_col2:
                            font_large = f'<p style="font-size: 18px;">{result['sentences_weight'][i]}</p>'
                            st.markdown(font_large, unsafe_allow_html=True)

                # -- Tab th√¥ng tin ƒë·ªô t∆∞∆°ng ƒë·ªìng
                with tab3:
                    ratio_tab3 = [1, 1, 3]
                    # Chia l√†m 3 c·ªôt, c·ªôt 1 v√† 2 l√† s·ªë th·ª© t·ª± c√¢u, c·ªôt 3 l√† th√¥ng tin ƒë·ªô t∆∞∆°ng ƒë·ªìng
                    header_tab3_col1, header_tab3_col2, header_tab3_col3 = st.columns(ratio_tab3, vertical_alignment="center")
                    with header_tab3_col1:
                        st.write("C√¢u")
                    with header_tab3_col2:
                        st.write("C√¢u")
                    with header_tab3_col3:
                        st.write("ƒê·ªô t∆∞∆°ng ƒë·ªìng c·ªßa c√¢u")
                        
                    for i in range(len(result['sentences_similarity'])):
                        index_sentence_1, index_sentence_2, similarity = result['sentences_similarity'][i]
                        tab3_col1, tab3_col2, tab3_col3 = st.columns(ratio_tab3, vertical_alignment="center")
                        with tab3_col1:
                            with st.popover(f"C√¢u {index_sentence_1 + 1}"):
                                st.write(result['sentences'][index_sentence_1])
                        
                        with tab3_col2:
                            with st.popover(f"C√¢u {index_sentence_2 + 1}"):
                                st.write(result['sentences'][index_sentence_2])
                            
                        with tab3_col3:
                            font_large = f'<p style="font-size: 18px;">{similarity}</p>'
                            st.markdown(font_large, unsafe_allow_html=True)
                
    # ====================== Evaluation Module ======================
    elif mode == mode_choice[2]:
        # Ch∆∞a g·ªçi API th√¨ hi·ªán c√°i v√≤ng tr√≤n l√™n
        if not st.session_state.api_data_inference['finish']:
            with st.spinner("ƒê·ª£i x√≠u, ƒëang l·∫•y d·ªØ li·ªáu"):
                st.session_state.api_data_inference['pos_tools'] = await api.get_pos_tools()
                st.session_state.api_data_inference['finish'] = True
        # ----- ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£ng
        # Th√™m vƒÉn b·∫£n
        st.subheader("T·∫£i l√™n t·∫≠p vƒÉn b·∫£n ƒë·ªÉ th·ª±c hi·ªán ƒë√°nh gi√°")
        file_hint_col1, file_hint_col2 = st.columns(2)
        with file_hint_col1:
            st.write("ƒê∆∞a v√†o t·∫≠p vƒÉn b·∫£ng theo ƒë·ªãnh d·∫°ng json")
        with file_hint_col2:
            with st.popover("B·∫•m v√†o ƒë·ªÉ xem d·ªãnh d·∫°ng json", use_container_width=True):
                st.markdown('''```json
[
    {
        'document': [{'topic':'ch·ªß ƒë·ªÅ c·ªßa vƒÉn b·∫£n 1', 'content':'n·ªôi dung c·ªßa vƒÉn b·∫£n 1'}, ...],
        'summary_content': ['vƒÉn b·∫£n t√≥m t·∫Øt 1', 'vƒÉn b·∫£n t√≥m t·∫Øt 2', ...]
    }
]''')
        document_uploaded_json = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files= False, key="document_uploaded")
        
        # -- Ch·ªçn tool ƒë·ªÉ th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i
        st.write("Ch·ªçn c√¥ng c·ª• th·ª±c hi·ªán g√°n nh√£n t·ª´ lo·∫°i")
        
        # G·ªçi api ƒë·ªÉ l·∫•y danh s√°ch c√°c tool c√≥ th·ªÉ s·ª≠ d·ª•ng
        pos_tools = st.selectbox("Ch·ªçn c√¥ng c·ª•", st.session_state.api_data_inference['pos_tools'])
        
        # ----- ƒê∆∞a v√†o m√¥ h√¨nh ch·ªß ƒë·ªÅ
        st.subheader("T·∫£i l√™n m√¥ h√¨nh (d·∫°ng json)")
        model_hint_col1, model_hint_col2 = st.columns(2)
        with model_hint_col1:
            st.write("M√¥ h√¨nh c√≥ d·∫°ng json")
        with model_hint_col2:
            with st.popover("B·∫•m v√†o ƒë·ªÉ xem d·ªãnh d·∫°ng json", use_container_width=True):
                st.markdown('''```json
{'topic_model':{'ch·ªß ƒë·ªÅ': [t·ª´ c·ªët l√µi]},
'word_similarity':{'type': 'lo·∫°i c√¥ng c·ª• cho ƒë·ªô t∆∞∆°ng t·ª± t·ª´', 
                    'word_similarity': 'k·∫øt qu·∫£ c·ªßa c√¥ng c·ª•',
                    'stop_words': 'danh s√°ch h∆∞ t·ª´ cho m√¥ h√¨nh'}}''')
        model_uploaded_json = st.file_uploader("T·∫£i t·ªáp l√™n", accept_multiple_files= False, key="model_uploaded")
        
        # ----- Nh·∫≠p c√°c th√¥ng s·ªë
        st.subheader("C√°c th√¥ng s·ªë t√≥m t·∫Øt")
        
        # T·ªâ l·ªá t√≥m t·∫Øt
        r_threshold = st.number_input("Ch·ªçn ƒë·ªô n√©n (t·ªâ l·ªá t√≥m t·∫Øt)", 0.0, 1.0, step=1.0, format="%.2f")
        r_threshold = round(r_threshold, 2)
        
        if st.button("B·∫Øt ƒë·∫ßu ƒë√°nh gi√° m√¥ h√¨nh"):
            with st.spinner("ƒêang th·ª±c hi·ªán ƒë√°nh gi√°"):
                results = await api.evaluate(document_uploaded_json,
                                            model_uploaded_json,
                                            {'r_threshold':r_threshold, 'pos_tool':pos_tools})
                for i in range(len(results)):
                    st.write(f"V·ªõi c√°ch t√≥m t·∫Øt th·ª© {i} th√¨ ph∆∞∆°ng ph√°p c√≥ gi√° tr·ªã c·ªßa c√°c ƒë·ªô ƒëo nh∆∞ sau:")
                    metric = pd.DataFrame.from_dict(results[i], orient='index', columns=['Max', 'Min', 'Mean'])
                    st.dataframe(metric)
if __name__ == "__main__":
    asyncio.run(main())